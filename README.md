

# Centralized Logging on Minikube (EFK) â€” Incident Log Search

## Context

In a Kubernetes environment, logs are one of the first places I check during an incident. But when applications are spread across pods and namespaces, troubleshooting becomes slow if I have to inspect logs one pod at a time.

For this project, I built a **centralized logging stack using EFK (Elasticsearch, Fluentd, Kibana)** on **Minikube**. The goal was to collect logs from across the cluster into one searchable place, so I can investigate issues faster during incidents.

---

## Problem

When an incident happens in Kubernetes, logs are usually scattered:

* Application logs live inside individual pods
* Pods can restart, which makes logs harder to track
* Multiple namespaces and services make investigation slower
* `kubectl logs` helps, but it is not enough for full incident response

During a real issue, I need to:

* search across many logs quickly
* filter by time
* isolate a namespace, pod, or container
* detect patterns like `error`, `timeout`, or `500`

The real problem is simple: **I need one central place to store and search logs across the cluster.**

---

## Solution

To solve that, I deployed an **EFK pipeline** on Minikube:

* **Fluentd** runs as a **DaemonSet** and collects logs from the node
* Fluentd forwards the logs to **Elasticsearch**
* **Kibana** connects to Elasticsearch and gives me a UI to search and filter logs

With this setup, I can follow a more practical incident workflow:

**Alert â†’ Open Kibana â†’ Search errors â†’ Identify affected pod/namespace â†’ Confirm root cause â†’ Validate fix**

This makes troubleshooting much faster than checking logs manually one by one.

---

## Architecture

![Architecture Diagram](screenshots/architecture.png)

This architecture shows how logs move through the cluster:

* application and system logs are generated by pods
* Fluentd collects them from the node
* Elasticsearch stores them
* Kibana lets me search, filter, and investigate them visually

This is the core of a simple centralized logging workflow for Kubernetes.

---

## Workflow

### 1. Start the Minikube cluster

**Goal:**
Prepare a local Kubernetes environment with enough resources to run Elasticsearch, Fluentd, and Kibana.

This is the foundation of the project because Elasticsearch needs more memory than a small default cluster usually has.

---

### 2. Create the logging namespace

**Goal:**
Separate logging components from application workloads so the project stays organized and easier to manage.

Using a dedicated namespace makes it clear which resources belong to the logging stack.

---

### 3. Deploy Elasticsearch

**Goal:**
Set up the storage engine where all collected logs will be indexed and stored.

Elasticsearch is the backend of the logging pipeline. If this is not healthy, the rest of the stack cannot work properly.

ðŸ“¸ **Screenshot:** `screenshots/01-efk-pods-running.png`
![EFK Pods Running](screenshots/01-efk-pods-running.png)

ðŸ“¸ **Screenshot:** `screenshots/02-elasticsearch-service.png`
![Elasticsearch Service](screenshots/02-elasticsearch-service.png)

---

### 4. Deploy Kibana

**Goal:**
Provide a visual interface to explore and search the logs stored in Elasticsearch.

Kibana is what turns raw logs into something I can use during an incident investigation.

ðŸ“¸ **Screenshot:** `screenshots/05-kibana-running.png`
![Kibana Running](screenshots/05-kibana-running.png)

---

### 5. Deploy Fluentd as a DaemonSet

**Goal:**
Collect logs from the node automatically and forward them into Elasticsearch.

Because Fluentd runs as a DaemonSet, it can collect logs consistently from the cluster without needing a separate collector for each app.

ðŸ“¸ **Screenshot:** `screenshots/03-fluentd-daemonset.png`
![Fluentd DaemonSet](screenshots/03-fluentd-daemonset.png)

---

### 6. Generate demo logs

**Goal:**
Simulate an incident by producing logs that contain useful patterns for testing, such as errors or noisy output.

This step proves that logs are actually being collected from workloads outside the logging namespace.

ðŸ“¸ **Screenshot:** `screenshots/04-demo-logs.png`
![Demo Logs](screenshots/04-demo-logs.png)

---

### 7. Access Kibana

**Goal:**
Open the Kibana interface and prepare it for investigation work.

This is the point where the logging stack becomes operational from a user perspective.

---

### 8. Create the data view in Kibana

**Goal:**
Tell Kibana how to read the indices so the incoming logs can be searched properly.

Without this step, the logs may exist in Elasticsearch, but they are not easy to explore in Kibana.

ðŸ“¸ **Screenshot:** `screenshots/06-index-pattern.png`
![Index Pattern](screenshots/06-index-pattern.png)

---

### 9. Search logs like an incident responder

**Goal:**
Use Kibana to search for error patterns, isolate the affected namespace or pod, and narrow down the incident timeline.

This is where the full value of centralized logging appears. Instead of guessing which pod to inspect first, I can search directly for known symptoms.

ðŸ“¸ **Screenshot:** `screenshots/07-kibana-error-search.png`
![Error Search](screenshots/07-kibana-error-search.png)

ðŸ“¸ **Screenshot:** `screenshots/08-filter-by-namespace.png`
![Filter by Namespace](screenshots/08-filter-by-namespace.png)

---

## Business Impact

This project improves troubleshooting and incident response in a Kubernetes environment.

Main impact:

* reduces time spent checking logs manually
* makes it easier to find the failing service during incidents
* gives one central place for cluster log investigation
* helps detect recurring patterns like errors, timeouts, and failed requests
* improves visibility across namespaces and workloads

From an operations point of view, this means faster diagnosis, faster recovery, and a more structured incident workflow.

---

## Troubleshooting

### Kibana opens but shows no logs

This usually means logs are not reaching Elasticsearch or Kibana is not reading the right index.

Things to verify:

* Fluentd is running correctly
* Elasticsearch is healthy
* indices are being created
* Kibana data view matches the index name

---

### Elasticsearch pod keeps restarting

This often happens because Minikube does not have enough memory allocated.

If Elasticsearch is unstable, the whole logging pipeline becomes unreliable.

---

### Fluentd is running but logs do not appear

This usually points to a Fluentd configuration issue or a forwarding problem between Fluentd and Elasticsearch.

Check whether:

* the ConfigMap is correct
* Fluentd logs show output errors
* Elasticsearch service is reachable from Fluentd

---

### Kibana cannot connect to Elasticsearch

This usually means Kibana is pointing to the wrong service name, the wrong port, or Elasticsearch is not ready yet.

---

### Elasticsearch port-forward does not work

This may happen if the service is missing, the pod is not healthy, or the local port-forward session is not active.

---

## Useful CLI

### General verification

```bash
kubectl get ns
kubectl get pods -n logging
kubectl get svc -n logging
kubectl get ds -n logging
kubectl get all -n logging
```

### Check Elasticsearch

```bash
kubectl logs -n logging deploy/elasticsearch --tail=100
kubectl get endpoints -n logging
kubectl port-forward -n logging svc/elasticsearch 9200:9200
curl -s http://localhost:9200
curl -s http://localhost:9200/_cat/indices?v
```

### Check Kibana

```bash
kubectl logs -n logging deploy/kibana --tail=100
kubectl describe pod -n logging -l app=kibana
kubectl port-forward -n logging svc/kibana 5601:5601
```

### Check Fluentd

```bash
kubectl logs -n logging ds/fluentd --tail=100
kubectl describe ds -n logging fluentd
kubectl get cm -n logging fluentd-config -o yaml
```

### Check demo app logs

```bash
kubectl get pods -n demo
kubectl logs -n demo deploy/demo-logger --tail=50
```

### Troubleshooting CLI

```bash
kubectl describe pod -n logging
kubectl describe svc -n logging
kubectl get events -n logging --sort-by=.metadata.creationTimestamp
kubectl top pods -n logging
minikube status
kubectl cluster-info
```

---

## Cleanup

```bash
kubectl delete ns demo --ignore-not-found
kubectl delete ns logging --ignore-not-found
minikube stop
```

If I want to remove the whole local cluster:

```bash
minikube delete
```

---
